{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Convolutional Networks\n",
    "#### Marohom, Alec Louie A. | 2019190226"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "Train a convolutional network that classifies the data in the provided dataset. Feel free to experiment with the layer architectures as long as it meets the requirements below. Perform the following after the training.\n",
    "\n",
    "Architecture requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)\tIf your first name starts with A-M, all pooling must be max pooling, else they must be average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\tA convolutional layer with NxN filter must exist (at least 1) where N is the number of vowels in your surname + 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c)\tOverall, the network should use at least 2 dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepData(df):\n",
    "    print('df shape: ', df.shape)\n",
    "    y = df['Label'].values #---------------- flat target | y[:5]\n",
    "    print('\\ny: \\n', y[:5])\n",
    "    y_encoder = OneHotEncoder(sparse=False)\n",
    "    y_reshape = y.reshape(-1, 1) #---------------- multidimensional target | y_encoded[:5]\n",
    "    y_encoded = y_encoder.fit_transform(y_reshape)\n",
    "    print('\\ny_encoded: \\n', y_encoded[:5])\n",
    "    imageInPixel = df.drop('Label', axis=1).values #---------------- pixel values\n",
    "    print('\\nimageInPixel: \\n', imageInPixel)\n",
    "    X = imageInPixel.reshape(-1, 28, 28, 1)\n",
    "    # imageInPixel = array of images\n",
    "    # X[a] = array of rows in image(a)\n",
    "    # X[a][b] = array of pixel values in row(b) in image(a)\n",
    "    # X[a][b][c] = value of pixel(c) in row(b) in image(a)\n",
    "    \n",
    "    return X, y_encoded, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------- Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv('utensils_train.csv')\n",
    "testdf = pd.read_csv('utensils_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label       int64\n",
       "Pixel0      int64\n",
       "Pixel1      int64\n",
       "Pixel2      int64\n",
       "Pixel3      int64\n",
       "            ...  \n",
       "Pixel779    int64\n",
       "Pixel780    int64\n",
       "Pixel781    int64\n",
       "Pixel782    int64\n",
       "Pixel783    int64\n",
       "Length: 785, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape:  (637, 785)\n",
      "\n",
      "y: \n",
      " [0 0 2 1 0]\n",
      "\n",
      "y_encoded: \n",
      " [[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "imageInPixel: \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train_encoded, y_train_raw = prepData(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape:  (71, 785)\n",
      "\n",
      "y: \n",
      " [2 1 2 0 1]\n",
      "\n",
      "y_encoded: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "\n",
      "imageInPixel: \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test_encoded, y_test_raw = prepData(testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    for i in range(20,25):\n",
    "        plt.imshow(X[i].reshape(28, 28))\n",
    "        plt.show()\n",
    "        print('Label:', y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------- Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 25, 25, 12)        204       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 12)        588       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 12)        588       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 12)        588       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 12)          588       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 12)          588       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 12)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               4900      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80)                8080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 243       \n",
      "=================================================================\n",
      "Total params: 16,367\n",
      "Trainable params: 16,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Architecture Requirements:\n",
    "# Alec Marohom\n",
    "# a) If your first name starts with A-M, all pooling must be max pooling, else they must be average pooling\n",
    "#    A - Max Pooling\n",
    "# b) A convolutional layer with NxN filter must exist (at least 1) where N is the number of vowels in your surname + 1.\n",
    "#    N = 4 | 4x4 filter\n",
    "\n",
    "input_ = tf.keras.Input((28, 28, 1))\n",
    "conv1 = tf.keras.layers.Conv2D(12, (4, 4), activation='relu')(input_)\n",
    "conv2 = tf.keras.layers.Conv2D(12, (2, 2), activation='relu')(conv1)\n",
    "mp1 = tf.keras.layers.MaxPool2D((2,2))(conv2)\n",
    "conv3 = tf.keras.layers.Conv2D(12, (2, 2), activation='relu')(mp1)\n",
    "conv4 = tf.keras.layers.Conv2D(12, (2, 2), activation='relu')(conv3)\n",
    "conv5 = tf.keras.layers.Conv2D(12, (2, 2), activation='relu')(conv4)\n",
    "conv6 = tf.keras.layers.Conv2D(12, (2, 2), activation='relu')(conv5)\n",
    "mp2 = tf.keras.layers.MaxPool2D((4,4))(conv6)\n",
    "fl = tf.keras.layers.Flatten()(mp2)\n",
    "\n",
    "dense1 = tf.keras.layers.Dense(100, activation='softmax')(fl)\n",
    "dense2 = tf.keras.layers.Dense(80, activation='relu')(dense1)\n",
    "output = tf.keras.layers.Dense(3, activation='softmax')(dense2) #---------------- softmax returns normalized prediction values | sums to 1\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=input_, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------- Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingAt(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get(self.monitor) <= self.value:\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = r'models/'\n",
    "bestFilePath = r'models/best_model/'\n",
    "\n",
    "cbList = [\n",
    "    EarlyStoppingAt(monitor='val_loss', value=0.01),\n",
    "    #tf.keras.callbacks.EarlyStopping(monitor='val_loss', baseline=2.00),\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',save_freq='epoch'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(bestFilePath, save_best_only=True)\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 637 samples, validate on 71 samples\n",
      "Epoch 1/35\n",
      "592/637 [==========================>...] - ETA: 0s - loss: 1.0758WARNING:tensorflow:From C:\\Users\\Alec\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/best_model/assets\n",
      "637/637 [==============================] - 2s 4ms/sample - loss: 1.0732 - val_loss: 1.0585\n",
      "Epoch 2/35\n",
      "632/637 [============================>.] - ETA: 0s - loss: 1.0460INFO:tensorflow:Assets written to: models/best_model/assets\n",
      "637/637 [==============================] - 2s 3ms/sample - loss: 1.0444 - val_loss: 1.0410\n",
      "Epoch 3/35\n",
      "608/637 [===========================>..] - ETA: 0s - loss: 1.0324INFO:tensorflow:Assets written to: models/best_model/assets\n",
      "637/637 [==============================] - 2s 3ms/sample - loss: 1.0331 - val_loss: 0.9885\n",
      "Epoch 4/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 1.0030 - val_loss: 1.0126\n",
      "Epoch 5/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.9757 - val_loss: 1.0035\n",
      "Epoch 6/35\n",
      "624/637 [============================>.] - ETA: 0s - loss: 0.9111INFO:tensorflow:Assets written to: models/best_model/assets\n",
      "637/637 [==============================] - 2s 3ms/sample - loss: 0.9118 - val_loss: 0.9616\n",
      "Epoch 7/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.8998 - val_loss: 1.0147\n",
      "Epoch 8/35\n",
      "592/637 [==========================>...] - ETA: 0s - loss: 0.8559INFO:tensorflow:Assets written to: models/best_model/assets\n",
      "637/637 [==============================] - 2s 3ms/sample - loss: 0.8589 - val_loss: 0.8962\n",
      "Epoch 9/35\n",
      "600/637 [===========================>..] - ETA: 0s - loss: 0.7996INFO:tensorflow:Assets written to: models/best_model/assets\n",
      "637/637 [==============================] - 2s 3ms/sample - loss: 0.8023 - val_loss: 0.8585\n",
      "Epoch 10/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.7963 - val_loss: 0.9941\n",
      "Epoch 11/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.7471 - val_loss: 1.0155\n",
      "Epoch 12/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.7291 - val_loss: 0.9728\n",
      "Epoch 13/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.7692 - val_loss: 0.9619\n",
      "Epoch 14/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.7081 - val_loss: 0.9463\n",
      "Epoch 15/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.7042 - val_loss: 0.9776\n",
      "Epoch 16/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.6602 - val_loss: 1.0347\n",
      "Epoch 17/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5887 - val_loss: 1.0455\n",
      "Epoch 18/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5983 - val_loss: 1.0257\n",
      "Epoch 19/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5833 - val_loss: 1.1789\n",
      "Epoch 20/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5478 - val_loss: 1.2504\n",
      "Epoch 21/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5306 - val_loss: 1.2804\n",
      "Epoch 22/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5115 - val_loss: 1.2316\n",
      "Epoch 23/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5511 - val_loss: 1.1722\n",
      "Epoch 24/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5467 - val_loss: 1.1852\n",
      "Epoch 25/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5640 - val_loss: 1.0960\n",
      "Epoch 26/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5790 - val_loss: 1.0381\n",
      "Epoch 27/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5139 - val_loss: 1.3191\n",
      "Epoch 28/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5025 - val_loss: 1.2355\n",
      "Epoch 29/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5193 - val_loss: 1.3202\n",
      "Epoch 30/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.4724 - val_loss: 1.2931\n",
      "Epoch 31/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.4766 - val_loss: 1.2710\n",
      "Epoch 32/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.4651 - val_loss: 1.2126\n",
      "Epoch 33/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5902 - val_loss: 0.9705\n",
      "Epoch 34/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.6376 - val_loss: 1.1401\n",
      "Epoch 35/35\n",
      "637/637 [==============================] - 1s 1ms/sample - loss: 0.5186 - val_loss: 1.0070\n"
     ]
    }
   ],
   "source": [
    "hst = model.fit(X_train, y_train_encoded, batch_size=8, epochs=35, validation_data=(X_test, y_test_encoded), callbacks=cbList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------- Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(bestFilePath)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Guess Accuracy: 33.3333 %\n",
      "Model Accuracy:  60.5634 %\n"
     ]
    }
   ],
   "source": [
    "print('Random Guess Accuracy: 33.3333 %')\n",
    "print('Model Accuracy: ', \"%.4f\" % (accuracy_score(y_test_raw, y_test_pred) * 100), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Precision, and Recall: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.87      0.71        23\n",
      "           1       0.57      0.19      0.29        21\n",
      "           2       0.61      0.70      0.66        27\n",
      "\n",
      "    accuracy                           0.61        71\n",
      "   macro avg       0.60      0.59      0.55        71\n",
      "weighted avg       0.60      0.61      0.57        71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Model Precision, and Recall: \\n')\n",
    "print(classification_report(y_test_raw, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(y_test_raw, y_test_pred), annot=True)\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Pred')\n",
    "print('Confusion Matrix: \\n')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
